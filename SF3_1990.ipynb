{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from census import Census\n",
    "from us import states\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import csv\n",
    "import matplotlib.pylab\n",
    "from functools import reduce\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert your census api key her below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"4c26aa6ebbaef54a55d3903212eabbb506ade381\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1990 SF3 Variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P0010001 - Total Population\n",
    "P0120001 - Total Non-Hispanic White Population\n",
    "P0120002 - Total Non-Hispanic Black Population\n",
    "P0080004 - Total Asian Population\n",
    "P0100001 - Total Latino Population\n",
    "\n",
    "P0170003 - Total Houselholds with Children\n",
    "\n",
    "P0420009 - Foreign Born\n",
    "\n",
    "H043A001 - Median Gross Rent\n",
    "\n",
    "H0040001 - Median value of owner occupied house\n",
    "\n",
    "H0010001 - Total Occupied Housing Units\n",
    "H0080001 - Owner Occupied Housing Units\n",
    "H0080002 - Renter Occupied Housing Units\n",
    "\n",
    "H0250001 - Units built from 1989 to 1990\n",
    "H0250002 - Units built from 1985 to 1988\n",
    "H0250003 - Units built from 1980 to 1984\n",
    "H0250004 - Units built from 1970 to 1979\n",
    "H0250005 - Units built from 1960 to 1969\n",
    "H0250006 - Units built from 1950 to 1959\n",
    "H0250007 - Units built from 1940 to 1949\n",
    "H0250008 - Units built before 1940\n",
    "\n",
    "P0490001 - Drove car alone\n",
    "\n",
    "Other means of transportation\n",
    "P0490002 + P0490003 + P0490004 + P0490005 + P0490006 + P0490007 + P0490008 + P0490009 + P0490010 + P0490011 + P0490012\n",
    "\n",
    "P0500001 + P0500002 + P0500003 + P0500004 + P0500005 + P0500006 - Travel time is less than 30 min\n",
    "P0500007 + P0500008 + P0500009 + P0500010 - Travel time is between 30 and 60 min\n",
    "P0500011 + P0500012 - Travel time is more than 60 min\n",
    "\n",
    "P0800001 - Less than 5k\n",
    "P0800002 - Between 5k - 10k\n",
    "P0800003 - Between 10k - 12.5k\n",
    "P0800004 - Between 12.5k - 15k\n",
    "P0800005 - Between 15k - 17.5k\n",
    "P0800006 - Between 17.5k - 20k\n",
    "P0800007 - Between 20k - 22.5k\n",
    "P0800008 - Between 22.5k - 25k\n",
    "P0800009 - Between 25k - 27.5k\n",
    "P0800010 - Between 27.5k - 30k\n",
    "P0800011 - Between 30k - 32.5k\n",
    "P0800012 - Between 32.5k - 35k\n",
    "P0800013 - Between 35k - 37.5k\n",
    "P0800014 - Between 37.5k - 40k\n",
    "P0800015 - Between 40k - 42.5k\n",
    "P0800016 - Between 42.5k - 45k\n",
    "P0800017 - Between 45k - 47.5k\n",
    "P0800018 - Between 47.5k - 50k\n",
    "P0800019 - Between 50k - 55k\n",
    "P0800020 - Between 55k - 60k\n",
    "P0800021 - Between 60k - 75k\n",
    "P0800022 - Between 75k - 100k\n",
    "P0800023 - Between 100k - 125k\n",
    "P0800024 - Between 125k - 150k\n",
    "P0800025 - 150k or more\n",
    "\n",
    "P080A001 - Median Household Income \n",
    "\n",
    "P0130018 - Age 25 to 29 years\n",
    "P0130019 - Age 30 to 34 years\n",
    "P0130020 - Age 35 to 39 years\n",
    "P0130021 - Age 40 to 44 years\n",
    "P0130022 - Age 45 to 49 years\n",
    "P0130023 - Age 50 to 54 years\n",
    "P0130024 - Age 55 to 59 years\n",
    "P0130025 - Age 60 to 61 years\n",
    "P0130026 - Age 62 to 64 years\n",
    "P0130027 - Age 65 to 69 years\n",
    "P0130028 - Age 70 to 74 years\n",
    "P0130029 - Age 75 to 79 years\n",
    "P0130030 - Age 80 to 84 years\n",
    "P0130031 - Age 85\n",
    "\n",
    "P0570006 - Total Population with a bachelor's Degree\n",
    "P0570007 - Total Population with a graduate Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the buckets for variables that are separeted into bins like income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = [0, 5000, 10000, 12500, 15000, 17500, 20000, 22500, 25000, 27500, 30000, 32500, 35000, 37500, 40000, \n",
    "           42500, 45000, 47500, 50000, 55000, 60000, 75000, 100000, 125000, 150000, 1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert the variables you want to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['H0040001', 'H0080001', 'H0080002', 'H043A001', 'H061A001', 'P0010001',\n",
    "             'P0060001', 'P0130018', 'P0130019', 'P0130020', 'P0130021', 'P0130022',\n",
    "             'P0130023', 'P0130024', 'P0130025', 'P0130026', 'P0130027', 'P0130028',\n",
    "             'P0130029', 'P0130030', 'P0130031', 'P0570006', 'P0800001', 'P0800002',\n",
    "             'P0800003', 'P0800004', 'P0800005', 'P0800006', 'P0800007', 'P0800008',\n",
    "             'P0800009', 'P0800010', 'P0800011', 'P0800012', 'P0800013', 'P0800014',\n",
    "             'P0800015', 'P0800016', 'P0800017', 'P0800018', 'P0800019', 'P0800020',\n",
    "             'P0800021', 'P0800022', 'P0800023', 'P0800024', 'P0800025', 'P080A001',\n",
    "             'P0120002', 'P0080004', 'P0080001', 'P0170003', 'P0420009', 'P0120001',\n",
    "             'P0120002', 'P0100001', 'P0570007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    var.append('P050000'+str(i))\n",
    "    var.append('P049000'+str(i))\n",
    "    if i<9:\n",
    "        var.append('H025000'+str(i))\n",
    "var.append('P0490010')\n",
    "var.append('P0490011')\n",
    "var.append('P0490012')\n",
    "var.append('P0500010')\n",
    "var.append('P0500011')\n",
    "var.append('P0500012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the state and insert its code (as a string number)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Alabama\t        01  AL\n",
    "Alaska\t        02  AK\n",
    "Arizona\t        04  AZ\n",
    "Arkansas\t    05  AR\n",
    "California\t    06  CA\n",
    "Colorado\t    08  CO\n",
    "Connecticut\t    09\tCT\n",
    "Delaware\t    10\tDE\n",
    "D.C.         \t11\tDC\n",
    "Florida\t        12\tFL\n",
    "Georgia\t        13\tGA\n",
    "Hawaii\t        15\tHI\n",
    "Idaho\t        16\tID\n",
    "Illinois\t    17\tIL\n",
    "Indiana\t        18\tIN\n",
    "Iowa\t        19\tIA\n",
    "Kansas\t        20\tKS\n",
    "Kentucky\t    21\tKY\n",
    "Louisiana\t    22\tLA\n",
    "Maine\t        23\tME\n",
    "Maryland\t    24\tMD\n",
    "Massachusetts\t25\tMA\n",
    "Michigan\t    26\tMI\n",
    "Minnesota\t    27\tMN\n",
    "Mississippi\t    28\tMS\n",
    "Missouri\t    29\tMO\n",
    "Montana\t        30\tMT\n",
    "Nebraska\t    31\tNE\n",
    "Nevada\t        32\tNV\n",
    "New Hampshire\t33\tNH\n",
    "New Jersey\t    34\tNJ\n",
    "New Mexico\t    35\tNM\n",
    "New York\t    36\tNY\n",
    "North Carolina\t37  NC\n",
    "North Dakota\t38  ND\n",
    "Ohio\t        39  OH \n",
    "Oklahoma\t    40  OK\n",
    "Oregon\t        41  OR\n",
    "Pennsylvania\t42  PA\n",
    "Rhode Island\t44  RI\n",
    "South Carolina\t45  SC\n",
    "South Dakota\t46  SD\n",
    "Tennessee\t    47  TN\n",
    "Texas\t        48  TX\n",
    "Utah\t        49  UT\n",
    "Vermont\t        50  VT\n",
    "Virginia\t    51  VA\n",
    "Washington\t    53  WA\n",
    "West Virginia\t54  WV\n",
    "Wisconsin\t    55  WI\n",
    "Wyoming\t        56  WY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = '09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query='state:{} county:*'.format(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = c.sf3.get(var, geo = {'for': 'tract:*',\n",
    "                       'in': sql_query}, year = 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query_2='state:{}'.format(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = c.sf3.get('P080A001', geo = {'for': 'county:*',\n",
    "                       'in': sql_query_2}, year = 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = pd.DataFrame(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hh_90'] = np.sum(df.iloc[:,62:87],axis=1)\n",
    "df['pop_over_25'] = np.sum(df.iloc[:,20:34],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('variables_' + state + '.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the thresholds for each of the counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = df.county.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = {}\n",
    "counties = counties.fromkeys(county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(columns)-5):\n",
    "    df[columns[i]] = df[columns[i]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = df_threshold.rename(columns={'P080A001':'median'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = df_threshold.set_index('county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = df_threshold.drop(columns='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_typo['vvli_threshold']=0.3*df_typo['median']\n",
    "df_threshold['vli_threshold'] = 0.5*df_threshold['median']\n",
    "df_threshold['li_threshold'] = 0.8*df_threshold['median']\n",
    "df_threshold['mi_threshold'] = 1.2*df_threshold['median']\n",
    "df_threshold['mhi_threshold'] = 1.5*df_threshold['median']\n",
    "df_threshold['hi_threshold'] = 2*df_threshold['median']\n",
    "df_threshold['vhi_threshold'] = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the different typologies based on each county's median income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.zeros((len(county),len(buckets)-1))\n",
    "for j in range(0, len(counties)):\n",
    "    col = 1\n",
    "    for i in range(0, len(buckets)-1):\n",
    "        if buckets[i+1] < df_threshold.iloc[j, col]:\n",
    "            table[j, i] = True\n",
    "        else:\n",
    "            table[j, i] = (df_threshold.iloc[j, col] - buckets[i]) / (buckets[i+1] - buckets[i])\n",
    "            col = col+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values=[]\n",
    "for i in county:\n",
    "    with open('variables_'+state+'.csv', 'r') as fi:\n",
    "        reader = csv.DictReader(fi)\n",
    "        def creator(reader, c):\n",
    "            bt = filter(lambda x: x['county'] == c, reader)\n",
    "            return (list(bt))\n",
    "        value = creator(reader, i)\n",
    "    values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_typo = []\n",
    "for j in range(0, len(county)):\n",
    "    for i in range(0, len(values[j])):\n",
    "        typo = []\n",
    "        summ = 0\n",
    "        for col in range(0,len(buckets)-1):\n",
    "            if ((table[j, col] == 1) & (values[j][i]['hh_90']!='0.0')):\n",
    "                summ = summ + float(values[j][i][var[22+col]])/float(values[j][i]['hh_90'])\n",
    "            else:\n",
    "                if values[j][i]['hh_90']!='0.0':\n",
    "                    summ = summ + float(values[j][i][var[22+col]])*table[j, col]/float(values[j][i]['hh_90'])\n",
    "                    typo.append(summ)\n",
    "                    summ = float(values[j][i][var[22+col]])*(1-table[j, col])/float(values[j][i]['hh_90'])\n",
    "        typo.append(summ)\n",
    "        income_typo.append(typo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(income_typo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.rename(columns={0:'vli_90', 1:'li_90', 2:'mi_90', 3:'mhi_90', 4:'hi_90', 5:'vhi_90'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commute_low_90'] = df['P0500001']+df['P0500002']+df['P0500003']+df['P0500004']+df['P0500005']+df['P0500006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commute_med_90'] = df['P0500007'] + df['P0500008'] + df['P0500009'] + df['P0500010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commute_high_90'] = df['P0500011'] + df['P0500012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commute_total'] = df['commute_high_90'] + df['commute_med_90'] + df['commute_low_90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transport_total'] = df['P0490002'] + df['P0490003'] + df['P0490004'] + df['P0490005'] + df['P0490006']\n",
    "df['transport_total'] = df['transport_total'] + df['P0490007'] + df['P0490008'] + df['P0490009'] + df['P0490010'] \n",
    "df['transport_total'] = df['transport_total'] + df['P0490011'] + df['P0490012'] + df['P0490001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['units_total'] = df['H0250001'] + df['H0250002'] + df['H0250003'] + df['H0250004'] + df['H0250005']\n",
    "df['units_total'] = df['units_total'] + df['H0250006'] + df['H0250007'] + df['H0250008']\n",
    "df['units_pre50'] = df['H0250008'] + df['H0250007'] \n",
    "df['units_80_90'] = df['H0250001'] + df['H0250002'] + df['H0250003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_90'] = df['P0570007'] + df['P0570006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_names = ['per_col_90', 'per_rent_90', 'per_owners_90', 'per_nonwhite_90','per_black_90', 'per_asian_90', \n",
    "             'per_latino_90', 'per_hhwchild_90',  'per_commute_low_90', 'per_commute_med_90', 'per_commute_high_90',\n",
    "             'per_foreign_90','per_car_commute_90', 'per_units_pre50', 'per_units_post80']\n",
    "\n",
    "moe_dividend = ['col_90', 'H0080002', 'H0080001', 'P0120001', 'P0120002', 'P0080004', 'P0100001', 'P0170003',\n",
    "               'commute_low_90', 'commute_med_90', 'commute_high_90', 'P0420009', 'P0490001', 'units_pre50',\n",
    "               'units_80_90']\n",
    "\n",
    "moe_divisor = ['pop_over_25', 'H0040001', 'H0040001', 'P0010001', 'P0010001', 'P0010001', 'P0010001', 'hh_90',\n",
    "              'commute_total', 'commute_total', 'commute_total', 'P0010001','transport_total', 'units_total', \n",
    "               'units_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(moe_names)):\n",
    "    if i!=3:\n",
    "        df_final[moe_names[i]] = df[moe_dividend[i]]/df[moe_divisor[i]]\n",
    "    else:\n",
    "        df_final[moe_names[i]] = 1 - df[moe_dividend[i]]/df[moe_divisor[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'H043A001':'mrent_90', 'P080A001':'hinc_90', \n",
    "                        'H061A001':'mhval_90', 'H0040001':'hu_90', 'P0010001':'pop_90'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.join(df[['hh_90', 'mrent_90', 'hinc_90', 'mhval_90', 'hu_90', 'pop_90', 'state', 'county',\n",
    "                             'tract']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Droping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_len = len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_len = len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of information lost is: 1.3189448441247003\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of information lost is: {}'.format((prev_len-final_len)*100/prev_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the SF3 1990 final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['GEOid2'] = df_final['state'] + df_final['county'] + df_final['tract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['GEOid2'] = df_final.GEOid2.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(state+'_1990_variables.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_len = len(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating the data to fit 2010 census tract boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90 = pd.read_csv('crosswalk_1990_2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90['state'] = df_crosswalk_90.trtid90.apply(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90_red = df_crosswalk_90[df_crosswalk_90.state==state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90_red = df_crosswalk_90_red.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90_red['trtid10'] = df_crosswalk_90_red['trtid10'].apply(lambda x: str(x))\n",
    "df_crosswalk_90_red['trtid90'] = df_crosswalk_90_red['trtid90'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "id90 = list(df_crosswalk_90_red.trtid90.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "id10 = list(df_crosswalk_90_red.trtid10.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning total weight sums that exceeds an stablished threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "weight_total = 0\n",
    "for i in id10:\n",
    "    weight_total = np.sum(df_crosswalk_90_red[df_crosswalk_90_red.trtid10==i].weight)\n",
    "    if ((weight_total<1-threshold)|(weight_total>1+threshold)):\n",
    "        id10.remove(i)\n",
    "        df_crosswalk_90_red = df_crosswalk_90_red[df_crosswalk_90_red.trtid10!=i]\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l=[]\n",
    "for i in range(0,len(df_crosswalk_90_red)):\n",
    "    l.append(df_crosswalk_90_red['weight'][i]/weight_total[df_crosswalk_90_red.trtid10[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_crosswalk_90_red['weight'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crosswalk_90_red = df_crosswalk_90_red.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final.GEOid2.isin(id10)].to_csv(state+'_1990_variables_clean.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_id10 = {}\n",
    "tract_id10 = tract_id10.fromkeys(id10)\n",
    "for i in range(0, len(df_crosswalk_90_red)):\n",
    "        if (type(tract_id10[df_crosswalk_90_red['trtid10'][i]])==list):\n",
    "            tract_id10[df_crosswalk_90_red['trtid10'][i]].append(df_crosswalk_90_red['trtid90'][i])\n",
    "        if (type(tract_id10[df_crosswalk_90_red['trtid10'][i]])!=list):\n",
    "            tract_id10[df_crosswalk_90_red['trtid10'][i]] = [df_crosswalk_90_red['trtid90'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dic = {}\n",
    "weight_dic = weight_dic.fromkeys(id10)\n",
    "for i in id10:\n",
    "    weight_dic[i] = weight_dic.fromkeys(tract_id10[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid = df_final[df_final.GEOid2.isin(id10)].GEOid2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(df_crosswalk_90_red)):\n",
    "    try:\n",
    "        if ((df_crosswalk_90_red['trtid90'][i] in geoid) & (df_crosswalk_90_red['weight'][i]!=0)):\n",
    "            weight_dic[df_crosswalk_90_red['trtid10'][i]][df_crosswalk_90_red['trtid90'][i]] = df_crosswalk_90_red['weight'][i]\n",
    "        elif ((len(weight_dic[df_crosswalk_90_red['trtid10'][i]])==1)|(df_crosswalk_90_red['weight'][i]!=0)):\n",
    "            weight_dic.pop(df_crosswalk_90_red['trtid10'][i])\n",
    "        else:\n",
    "            weight_dic[df_crosswalk_90_red['trtid10'][i]].pop(df_crosswalk_90_red['trtid90'][i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross(dictionary, reader, key, weight):\n",
    "    with open(state+'_1990_variables_clean.csv', 'r') as fi:\n",
    "        reader = csv.DictReader(fi)\n",
    "        bt = list(filter(lambda x: x['GEOid2'] == key, reader))\n",
    "        nt = np.fromiter(bt[0].values(), dtype = float)[:-4]*weight\n",
    "    return (nt)\n",
    "arr = map(lambda j: reduce((lambda x,y: x+y),(map(lambda k: cross(weight_dic[j], \n",
    "                                              reader, k, weight_dic[j][k]), \n",
    "                                             list(map(lambda x: x, weight_dic[j]))))), \n",
    "                                             list(map(lambda j: j, weight_dic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_final_crosswalk = pd.DataFrame(list(arr), columns = df_final.iloc[:,:-4].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_crosswalk['GEOid2'] = list(map(lambda j: j, weight_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_crosswalk_len = len(df_final_crosswalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The percentage of information lost is: {}'.format((df_final_len-df_final_crosswalk_len)*100/df_final_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_crosswalk.to_csv(state+'_1990_variables_crosswalk.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_crosswalk.iloc[:,:6].sum(axis=1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
